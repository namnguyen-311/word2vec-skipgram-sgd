# word2vec-skipgram-sgd
Manual implementation of Skip-Gram Word2Vec with Negative Sampling and Stochastic Gradient Descent from Stanford CS224N Lecture 2.

# Word2Vec Skip-Gram (Manual Implementation)

This is a simple Python implementation of **one training step** of the **Skip-Gram Word2Vec model with Negative Sampling**, inspired by Stanford's CS224N (Lecture 2).

## What It Does
- Computes dot products between word vectors
- Applies sigmoid function
- Calculates the loss using negative sampling
- Computes gradient w.r.t the center word vector
- Updates the vector using Stochastic Gradient Descent (SGD)

## Concepts Covered
- Word Embeddings
- Skip-Gram Model
- Negative Sampling
- SGD (Stochastic Gradient Descent)
- Dot Product & Sigmoid
- Loss Function Optimization

## Output Example
